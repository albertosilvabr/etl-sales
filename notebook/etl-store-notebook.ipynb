{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "759cc368-f6ef-4542-8c47-57bbe1d3226b",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2837e2e9-bb52-481a-bab3-f97cb53c7944",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de0cf26-fa1c-4baa-bfcf-ad002ae6c941",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"etl_sales\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc3f7bf-35e6-4c60-829b-dd2e0e9be8ae",
   "metadata": {},
   "source": [
    "# 2. Funções, Variaves e Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d73f4-25f1-4fb7-a4aa-ae85e6deed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tables = [\"customer\",\"employee\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bff5c24-6251-49b6-a57c-24694fad29a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_csv = path_landing + \"*.csv\"\n",
    "path_landing = \"/home/jovyan/work/dataset/store/landing/\"\n",
    "path_normalization = \"/home/jovyan/work/dataset/store/normalization/\"\n",
    "path_consolidation = \"/home/jovyan/work/dataset/store/consolidation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6f0034-4c36-40d2-9185-6d39613faf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTempView(path, table):\n",
    "    df = spark.read.parquet(path)\n",
    "    df.createOrReplaceTempView(table)\n",
    "    \n",
    "def q(query, n=30):\n",
    "    return spark.sql(query)    \n",
    "\n",
    "def readParquet(path):\n",
    "    return spark.read.format(\"parquet\").load(path)\n",
    "\n",
    "def saveParquet(df, path):\n",
    "    df.write.mode(\"overwrite\").format(\"parquet\").save(path)\n",
    "    \n",
    "def readCSV(path):\n",
    "    df = (\n",
    "        spark\n",
    "        .read\n",
    "        .format(\"csv\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"inferSchema\", True)\n",
    "        .option(\"delimiter\", \";\")\n",
    "        .load(path)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba62bd9-e4c9-4994-8ea7-76b0269b07cd",
   "metadata": {},
   "source": [
    "# 2. De Landing para Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c1ae94-2dca-403d-a40e-0e543ae95f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = readParquet(path_normalization + \"customer\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006d5e9c-1763-40b1-8a53-7be01158312f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in glob.glob(path_csv):  \n",
    "\n",
    "    file_name = os.path.basename(file).replace(\".csv\",\"\")\n",
    "\n",
    "    df_csv = readCSV(file) \n",
    "\n",
    "    if list_tables.count(file_name):\n",
    "        df_csv = df_csv.withColumn(\"gender\", when(col(\"gender\") == \"F\", \"Masculino\").otherwise(\"Feminino\"))    \n",
    "        saveParquet(df_csv, path_normalization + file_name)\n",
    "        \n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44d388b-6b7a-4a28-945f-9c9eb5702b2f",
   "metadata": {},
   "source": [
    "# 4. De Normalization para Consolidation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e159b4-e3ae-4073-aff8-8230e2374226",
   "metadata": {},
   "source": [
    "- Tabela Fato **Sales**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdaf66f-79bd-49e2-ac7c-73d35755bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "createTempView(path_normalization + \"sale\", \"sale\")\n",
    "createTempView(path_normalization + \"sale_item\", \"sale_item\")\n",
    "createTempView(path_normalization + \"product\", \"product\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec9250-f248-43f0-97e2-7a5156885c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fact_sales = spark.sql(\"\"\"\n",
    "        SELECT s.id sale_key\n",
    "             , s.id_customer AS customer_key\n",
    "             , s.id_branch   AS branch_key\n",
    "             , s.id_employee AS employee_key\n",
    "             , p.id          AS product_key\n",
    "             , si.quantity \n",
    "             , p.cost_price \n",
    "             , p.sale_price \n",
    "             , (si.quantity * p.cost_price) AS amount_cost_price\n",
    "             , (si.quantity * p.sale_price) as amount_sale_price\n",
    "        FROM sale s INNER JOIN sale_item si ON s.id  = si.id_sale \n",
    "                    INNER JOIN product p    ON p.id  = si.id_product          \n",
    "        ORDER BY s.id ;             \n",
    "\"\"\")\n",
    "\n",
    "saveParquet(df_fact_sales, path_consolidation + \"fact-sales\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eed33c7-74fc-4f4b-8ed6-30804974f923",
   "metadata": {},
   "source": [
    "- Tabela Dimensão **Product**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4c2340-6c78-451e-984f-71d0f1531829",
   "metadata": {},
   "outputs": [],
   "source": [
    "createTempView(path_normalization + \"product\", \"product\")\n",
    "createTempView(path_normalization + \"product_group\", \"product_group\")\n",
    "createTempView(path_normalization + \"supplier\", \"supplier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15098a24-ab3f-4f2f-937b-947770bc4b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_product = spark.sql(\"\"\"\n",
    "    SELECT p.id    as product_key\n",
    "         , p.name  AS product_name\n",
    "         , pg.name AS product_group_name\n",
    "         , s.name  AS supplier_name\n",
    "      FROM product p INNER JOIN product_group pg ON p.id_product_group = pg.id\n",
    "                     INNER JOIN supplier s       ON p.id_supplier      = s.id\n",
    "    ORDER BY p.id;   \n",
    "\"\"\")\n",
    "\n",
    "saveParquet(df_dim_product, path_consolidation + \"dim-product\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8a0cd1-171f-44fb-b342-7d2c71a304ec",
   "metadata": {},
   "source": [
    "- Tabela Dimensão **Customer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ce0e42-50e9-40bc-a061-c271137e55ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "createTempView(path_normalization + \"customer\", \"customer\")\n",
    "createTempView(path_normalization + \"district\", \"district\")\n",
    "createTempView(path_normalization + \"city\", \"city\")\n",
    "createTempView(path_normalization + \"state\", \"state\")\n",
    "createTempView(path_normalization + \"zone\", \"zone\")\n",
    "createTempView(path_normalization + \"marital_status\", \"marital_status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93834ffb-42bc-40f0-98fd-b9e5bc425b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_customer = spark.sql(\"\"\"\n",
    "    SELECT c.id    AS  customer_key\n",
    "         , c.name  AS  customer_name\n",
    "         , c.income \n",
    "         , c.gender\n",
    "         , d.name  AS district_name\n",
    "         , m.name  as marital_status\n",
    "         , z.name  AS zone_name\n",
    "         , cit.name AS city_name\n",
    "         , s.name  AS state_name\n",
    "      FROM customer c INNER JOIN district d       ON d.id   = c.id_district               \n",
    "                      INNER JOIN city cit         ON cit.id = d.id_city\n",
    "                      INNER JOIN state s          ON s.id   = cit.id_state\n",
    "                      INNER JOIN zone z           ON z.id   = d.id_zone\n",
    "                      INNER JOIN marital_status m ON m.id   = c.id_marital_status\n",
    "   ORDER BY c.id;        \n",
    "        \"\"\")\n",
    "\n",
    "saveParquet(df_dim_customer, path_consolidation + \"dim-customer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa14c3b6-3f01-4189-a76e-d2d6da392937",
   "metadata": {},
   "source": [
    "- Tabela Dimensão **Branch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40071683-1c29-4a25-b543-834963717c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "createTempView(path_normalization + \"branch\", \"branch\")\n",
    "createTempView(path_normalization + \"district\", \"district\")\n",
    "createTempView(path_normalization + \"city\", \"city\")\n",
    "createTempView(path_normalization + \"state\", \"state\")\n",
    "createTempView(path_normalization + \"zone\", \"zone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13aa2ea3-3582-4e27-bad3-23238925403e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_branch = spark.sql(\"\"\"\n",
    "    SELECT bc.id    AS branch_key\n",
    "         , bc.name  AS branch_name\n",
    "         , dt.name  AS district_name\n",
    "         , z.name   AS zone_name\n",
    "         , cit.name AS city_name\n",
    "         , s.name   AS state_name\n",
    "      FROM branch bc  INNER JOIN district dt ON dt.id  = bc.id_district              \n",
    "                      INNER JOIN city cit    ON cit.id = dt.id_city\n",
    "                      INNER JOIN state s     ON s.id   = cit.id_state\n",
    "                      INNER JOIN zone z      ON z.id   = dt.id_zone\n",
    "      ORDER BY bc.id;    \n",
    "\"\"\")\n",
    "\n",
    "saveParquet(df_dim_branch, path_consolidation + \"dim-branch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd005a98-f406-4fa9-ba9d-02b139760900",
   "metadata": {},
   "source": [
    "- Tabela Dimensão **Employee**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985ccad8-fabe-4d48-a90d-df1356d022f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "createTempView(path_normalization + \"employee\", \"employee\")\n",
    "createTempView(path_normalization + \"department\", \"department\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0028ecdb-2b37-4a59-aa51-08aed376a418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_employee = spark.sql(\"\"\"\n",
    "    SELECT e.id    AS  employee_key\n",
    "         , e.name  AS  employee_name\n",
    "         , d.name  AS  department_name       \n",
    "      FROM employee e INNER JOIN department d ON d.id = e.id_department\n",
    "     ORDER BY e.id;     \n",
    "\"\"\")\n",
    "\n",
    "saveParquet(df_dim_employee, path_consolidation + \"dim-employee\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eb50f0-a883-40ed-8a83-e31852e63bfa",
   "metadata": {},
   "source": [
    "# Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e977317-de73-4a29-aed4-0fe0b39a85db",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
